---
title: Xnorm and Code
date: 2024-10-12 11:30:00 +0800
categories: [machine learning]
tags: [deep learning]     # TAG names should always be lowercase
math: true
---

# Xnorm
规范化（Normalization）的核心是为了让不同层的输入取值范围或者分布能够比较一致。

在堆叠式的神经网络中，高层的网络会受到之前所有底层网络参数变化的影响，导致该高层网络的输入的分布产生较大的改变，这种现象被称为内部协变量偏移（Internal Covariate Shift）。随着网络深度的增大，这种差异会变得更加显著，从而影响模型的训练速度和最终性能。

输入分布变化较大会导致反向传播的梯度在不同层之间波动较大，这主要是因为输入分布的变化会影响到激活函数的输出和梯度的传播。如果某一层的输入分布变化较大，导致激活函数的导数在某些区域非常小（如Sigmoid在输入远离0时的导数接近0），那么反向传播的梯度会被大幅缩小，反之亦然。
## 1. Batchnorm
BN是在一个mini-batch上进行规范化，
- (1) 给定样本$Z_i$，其中$i=1,2,...,m$
- (2) 计算样本的第$j$维度特征的均值$\mu_j=\frac{1}{m}\sum_{i=1}^{m}Z^i_j$
- (3) 计算样本的第$j$维度特征的方差$\sigma_j=\frac{1}{m}\sum_{i=1}^{m}(Z^i_j-\mu_j)^2$
- (4) 样本的每个维度减去均值，除以标准差$\hat{Z}_j^i=\frac{Z^i_j-\mu_j}{\sqrt{\sigma_j+\epsilon}}$

BN在mini-batch时不能发挥作用；在训练时，Batch Norm 需要保存每个 step 的统计信息（均值和方差）。在测试时，由于变长句子的特性，测试集可能出现比训练集更长的句子，所以对于后面位置的 step，是没有训练的统计量使用的；
注意在BatchNorm中，用于更新running_var时，使用无偏样本方差，但是在对batch进行归一化时，使用有偏样本方差，因此如果batch_size=1，更新running_var时会报错

    ```python
    #!/usr/bin/env python3
    torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    #输入维度是(N, C, L)时，num_features应该取C；这里N是batch size，C是数据的channel，L是数据长度，输入维度是(N, L)时，num_features应该取L；
    #momentum更新全局均值running_mean和方差running_var时使用该值进行平滑
    #affine设为True时，BatchNorm层才会学习参数$\gamma$和$\beta$，否则不包含这两个变量，变量名是weight和bias
    #track_running_stats设为True时，BatchNorm层会统计全局均值running_mean（初值是[0., 0., 0., 0.]）和方差running_var[1., 1., 1., 1.]）
    m = nn.BatchNorm1d(100)
    m = nn.BatchNorm2d(100)
    ```
## 2. Layer Norm
Layer Norm即“Layer Norm”。

## 3. Instance Normalization

## 4. RMSNorm
[RMSNorm论文阅读](https://mltalks.medium.com/rmsnorm%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-bfae83f6d464)